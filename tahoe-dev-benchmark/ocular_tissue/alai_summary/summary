Summary of Execution of Pig_lens3 files
Adrian Lai (Mechanical Engineering, Cornell 2006)
Date: August 18th, 2005

This document contains information regarding some procedures when performing my lens compression simulation. It is not a comprehensive instruction on the whole process, but focuses more on how I dealt with common errors that I encountered.

CUBIT

Export Overwrite
Unlike other more commercial software, Cubit does not prompt the user for permission to overwrite when it tries to export a geometry file with a filename that already exists. Instead, it simply aborts and not exports the file.
Remember to add the option ‘overwrite’ when exporting a meshed model. The GUI interface does not seem to provide an overwrite option; therefore it needs to be done in the command line e.g. 

XML EDITING / XAMPLE

You should use Xample to produce the template file initially, to avoid having to write up all the syntax yourself. However, it is advised that further modifications be performed in vi or other text editors to edit the script instead of Xample. This is because every time you save a file in Xample, it reformats the *.xml or *.xml.tmpl into a format which is not necessarily compatible with Dakota.

Variables (those which substituted by variables in a DAKOTA run) in the xml.tmpl file:
- must be on separate lines in the template file
- must not have a name from which another variable name is embedded
o For example, if in the template file you have two variables, TAU and TAUT - DAKOTA replaces all instances of TAU with the value 100. As a result, TAU becomes 100, and TAUT becomes 100T. This would clearly not be the intention. It would therefore be better in this case to name the variables TAU0 and TAUT, where no one variable is found in the subset of the characters of another.


Editing the files

Error_func.csh

Set the Tahoe and Dakota paths to your current builds.

Pstudy.in
The pstudy.in file is a parameter study file used to run tests for sets of user-specified parameters. This is useful for testing numbers of varying magnitudes to find the ballpark figures before proceeding to use patsearch.in.

Check that the number of variables is consistent throughout the file (partitions, continuous_design, cdv_descriptor, cdv_lower_bounds, cdv_upper_bounds)

Patsearch.in
Make sure lower bound < initial point < upper bound. Use this to narrow down and find the optimal parameters after you get the ballpark figures in pstudy.in. 

Check that the number of variables is consistent throughout the file (continuous_design, cdv_descriptor, cdv_initial_point, cdv_lower_bounds, cdv_upper_bounds)


Experimental_data
Check data to make sure there are no two data points which share the same x-value
- E.g. a data set such as this would fail in Dakota
2997.5-0.150.152998-0.120.122998.5-0.140.142998.5-0.140.142999-0.150.152999.5-0.140.143000-0.160.16

Running DAKOTA

Dakota Optimization
Before performing optimization / simulations, check that all the following files are present:
- Dakota input file (e.g. pstudy.in, patsearch.in)
- Error function script (error_func.csh)
- Geometry file (e.g. pig_lens3.g pig_lens3_mc.g)
- Tahoe input template file (e.g. pig_len3_serial.xml.tmpl)
- Experimental data file (e.g. experimental_data)


Parallel Processing - TAHOE

It is possible to run Tahoe in parallel.
sim mpirun –np 4 tahoe –f filename.xml
Number of nodes, shortcut name for tahoe executable and filename should be changed as appropriate.  
As of now, contact penalties do not compute properly when running Tahoe in parallel.
Wall Penalty, however, works in parallel fine.


It is also possible to run Dakota in parallel
sim mpirun –np 4 dakota –i pstudy.in

Make sure to give dakota enough parameters to study simultaneously otherwise dakota would fail. If the optimization process is very short, or there are very few sets of parameters to test in pstudy.in, parallel processing should not be used. Simply use sim dakota –i pstudy.in

Due to the fact that contact penalties do not work when running Tahoe parallel and running Dakota and Tahoe both in parallel don’t work, I decided to stick with using Tahoe in serial and running Dakota in parallel.


Gnuplot

To compare the load-time graphs generated by the simulation (the *.fvst_Y files) with the experimental data, run Gnuplot and type the following command in Gnuplot.
Plot “experimental_data”, “error_func.in.1.fvst_Y” w l
It is helpful to join the points of simulated data with a line to produce a smooth loading curve. This is done by the above “w l” option.
However, with the experimental_data, there is a lot of fluctuation and noise in the data and joining the dots with a line only makes the graphs less clear.
